
# Compliance RAG System – End-to-End Demo (Jupyter Notebook)
# ----------------------------------------------------------
# This notebook demonstrates all key steps of the RAG system:
# 1. Loading documents
# 2. Creating embeddings
# 3. Building the FAISS index
# 4. Asking questions to the vector store + LLM

# ✅ Setup
!pip install -q langchain langchain-community langchainhub sentence-transformers faiss-cpu openai

import os
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from pathlib import Path

# ✅ Load and preview raw documents
raw_docs_path = Path("data/raw")
docs = []

for file in raw_docs_path.glob("*"):
    if file.suffix.lower() == ".txt":
        with open(file, "r", encoding="utf-8") as f:
            content = f.read()
        docs.append(Document(page_content=content, metadata={"source": file.name}))

print(f"Loaded {len(docs)} documents")
print("Preview:")
print(docs[0].page_content[:500])

# ✅ Split text for embedding
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
all_chunks = splitter.split_documents(docs)
print(f"Generated {len(all_chunks)} chunks")

# ✅ Create embeddings and store in FAISS
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-albert-small-v2",
    encode_kwargs={"normalize_embeddings": True}
)

index_path = "data/index"
db = FAISS.from_documents(all_chunks, embedding_model)
db.save_local(index_path)
print(f"Saved FAISS index to '{index_path}'")

# ✅ Load FAISS and set up retriever
db = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)
retriever = db.as_retriever()

# ✅ Set up lightweight LLM (Ollama must be running locally)
llm = Ollama(model="tinyllama")

# ✅ Ask questions
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff"
)

question = "What is the most recent FATF recommendation?"
response = qa.run(question)
print("Question:", question)
print("Answer:", response)
