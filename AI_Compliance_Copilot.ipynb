{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995933b4",
   "metadata": {},
   "source": [
    "#### Notebook Structure\n",
    "\n",
    "**Part I: Compliance Copilot Report**  \n",
    "This section is a concise strategic report covering the proposal’s justification, solution overview, key results discussion, identified challenges, conclusion and next steps.\n",
    "\n",
    "**Part II: Compliance Copilot Demo**  \n",
    "This section walks through the RAG pipeline steps, prints example outputs, and finally launches the terminal-based Copilot for live testing with a set of curated Q&A examples.\n",
    "\n",
    "**README.md**  \n",
    "Contains full technical details on installing dependencies, running the scalable pipeline code, and other execution instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1686b322",
   "metadata": {},
   "source": [
    "## Part I: Compliance Copilot Report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3466e",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "This section presents the **business rationale**, **strategic justification**, and **solution proposal** for the **Dow Jones AI Compliance Copilot**—a tool designed to **add an intelligence layer** to compliance data that Dow Jones already licenses, processes, and delivers to B2B clients. By embedding these datasets into an RAG-powered Q&A interface, clients can:\n",
    "\n",
    "- Receive instant, source-cited answers in natural language\n",
    "- Save dozens of weekly hours on due diligence\n",
    "- Reduce the risk of missing critical insights\n",
    "- Make faster, more accurate decisions\n",
    "\n",
    "Financially, this intelligence layer can **significantly increase the perceived value** of licensed data, unlocking new upsell and recurring revenue opportunities.\n",
    "\n",
    "> **Note:** In this POC, some answers may seem “obvious” or easily found with a web search, which can give the impression of limited value. In a production scenario, however, the Copilot would be powered by Dow Jones’ **exclusive, proprietary data**—meaning every conversational response delivered is unique to the customer’s licensed content and cannot be replicated by general-purpose search engines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f8f43",
   "metadata": {},
   "source": [
    "### 2. Strategic Justification & Market Opportunity\n",
    "\n",
    "#### 2.1 Why B2B Compliance First?\n",
    "- The **Governance, Risk & Compliance (GRC)** market is valued at **$62.9 billion** in 2024 with a **13.2% CAGR** through 2030 [1].\n",
    "- The **RegTech** subsector grows from **$4.7 billion** in 2024 to **$29 billion** by 2034 at a **20% CAGR** [2].\n",
    "- B2B compliance solutions command **ARPU of $50K–$500K/year**, far above B2C offerings [3].\n",
    "\n",
    "Focusing initially on corporate compliance clients maximizes ROI, tapping a fast-growing market with buyers willing to invest in high-value solutions.\n",
    "\n",
    "### 2.2 Existing Dow Jones Offerings\n",
    "- **RiskFeeds**: Structured feeds (sanctions, PEPs, adverse media) in **XML, CSV, JSON** formats for screening and reporting [4].\n",
    "- **Integrity Check (with Xapien)**: Entity due diligence reports based on entity name and country, cutting analysis from **days to minutes** [5].\n",
    "\n",
    "**How the Copilot Differs**:\n",
    "- **Complementary** to RiskFeeds and Integrity Check, adding open-ended Q&A across all licensed data.\n",
    "- Returns **source-cited answers** instead of fixed reports or raw feeds.\n",
    "- Provides **ad hoc insights** to support immediate decision-making, beyond standard outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad35f06",
   "metadata": {},
   "source": [
    "### 3. Solution Proposal\n",
    "\n",
    "#### 4.1 Data Acquisition\n",
    "- **Production**: Clients continue to license **proprietary Dow Jones data** (sanctions, PEPs, filings, adverse media) via XML/CSV/JSON and APIs.\n",
    "- **POC**: For this proof of concept, **publicly sourced documents** were extracted and processed from the internet to simulate the same data structure.\n",
    "\n",
    "#### 4.2 Pipeline\n",
    "```plaintext\n",
    "1. Licensed Dow Jones datasets\n",
    "2. Ingestion & Parsing (PDF, CSV/XML, HTML)\n",
    "3. Vector Indexing (embeddings)\n",
    "4. Agent AI + LLM (semantic search + generation)\n",
    "5. Source-cited answers in Q&A interface\n",
    "```\n",
    "\n",
    "#### 4.3 Key POC Components\n",
    "- **RAG**: Combines vector search with LLM for grounded responses.\n",
    "- **Prompt Engineering**: Domain-specific prompt tuning.\n",
    "- **Model Flexibility**: Support for GPT-4, Claude, and on-premise Ollama models.\n",
    "\n",
    "#### 4.4 README & Usage Instructions\n",
    "The **README** in the repository guides:\n",
    "- **Installation & Setup**: Virtual environment, dependencies, local models.\n",
    "- **Pipeline Execution**: Document download (`docs/fetch_data.py`), semantic index build (`src/build_index.py`), CLI Q&A (`main.py`, `backend.py`).\n",
    "- **Notebook Demo**: `compliance_demo_report.ipynb` walks through interactive tests with example questions and source-cited answers.\n",
    "\n",
    "#### 4.5 Test Questionnaire\n",
    "Because local indexing was resource-intensive and only a small document set was indexed, the file **`compliance_suggestion_questions.md`** includes curated test questions referencing the indexed context. This accelerated testing by covering scenarios such as:\n",
    "- Entity mention lookups in sanctions and reports\n",
    "- Queries on recent regulatory recommendations\n",
    "- Verifications of company-risk alerts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b65260",
   "metadata": {},
   "source": [
    "### 4. Next Steps\n",
    "1. **Infrastructure**: Migrate local storage to AWS S3/Azure Blob/GCP Buckets.\n",
    "2. **OCR Integration**: Add Tesseract or Amazon Textract for scanned PDFs.\n",
    "3. **Structured Validation**: Define precision/recall metrics and gather expert feedback.\n",
    "4. **Deployments**: REST API (FastAPI), front-end (React/Streamlit), cloud deployment (Vercel, Hugging Face).\n",
    "5. **Multi-tenant & Customization**: Client-specific histories and permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787a5b4",
   "metadata": {},
   "source": [
    "### 6. Conclusion\n",
    "The **Dow Jones AI Compliance Copilot** introduces a conversational intelligence layer atop already licensed data, delivering rapid, cited, and contextual insights. This evolution drives **new recurring revenue**, **higher margins**, and **enhances Dow Jones' leadership** in the RegTech space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a0933",
   "metadata": {},
   "source": [
    "### References\n",
    "1. Grand View Research, “eGRC Market Size, Share & Trends Analysis Report,” 2024.\n",
    "2. Mordor Intelligence, “RegTech Market Forecast to 2034.”\n",
    "3. Gartner, “Enterprise GRC Buyer Insights,” 2023.\n",
    "4. The Wealth Mosaic, “Dow Jones RiskFeeds Overview.”\n",
    "5. Xapien & Dow Jones Partnership Announcement, 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91343257",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb16992",
   "metadata": {},
   "source": [
    "## Part 2: Compliance Copilot Demo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ef17f",
   "metadata": {},
   "source": [
    "### 1. Best Practices for Demo Usage\n",
    "\n",
    "- **Ask objective, specific questions** related to compliance.  \n",
    "- **Avoid vague or overly broad questions.**  \n",
    "- **Verify any uncertain or ambiguous answers**, as the system is restricted to indexed content and is parameterized to minimize hallucinations—watch for any extrapolation.  \n",
    "- **Consult `compliance_suggestion_questions.md`** for a sample of supported questions.  \n",
    "\n",
    "_In this demo notebook, you will first see a concise overview of the pipeline steps and outputs. After that, you’ll test the Copilot interactively via the terminal interface._ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9b7770",
   "metadata": {},
   "source": [
    "### 2. Load and Preview Raw Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dab43e",
   "metadata": {},
   "source": [
    "The `docs/fetch_data.py` simulates how compliance documents would be ingested for the POC by:\n",
    "\n",
    "1. **Downloading** various public filings, advisories, and sanction lists (SEC, OFAC, FATF, FinCEN, EU) into `data/raw`.  \n",
    "2. **Handling failures** (403/404) by adding files manually when needed.  \n",
    "3. **Note**: In a real deployment, documents (PDF, CSV, TXT, etc.) would be loaded directly from authenticated internal storage or cloud buckets, not scraped from public websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1ee5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " === Raw files: ===\n",
      "- apple_10k_2022.txt\n",
      "- eu_annex2_sanctions.pdf\n",
      "- fatf_annual_report_2022-2023.pdf\n",
      "- fatf_assessment_methodology_2022.pdf\n",
      "- fatf_effectiveness_compliance_report_2022.pdf\n",
      "- fatf_procedures_mutual_evaluations_2022.pdf\n",
      "- fatf_universal_procedures.pdf\n",
      "- fincen_advisory_corruption_2022.pdf\n",
      "- fincen_advisory_elder_exploitation_2022.pdf\n",
      "- fincen_alert_pig_butchering_2023.pdf\n",
      "- fincen_alert_russian_elites_2022.pdf\n",
      "- fincen_ransomware_advisory.pdf\n",
      "- microsoft_10k_2022.txt\n",
      "- ofac_sdn_list.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "RAW_PATH = \"data/raw\"\n",
    "print(\" === Raw files: ===\")\n",
    "for fname in os.listdir(RAW_PATH):\n",
    "    print(\"-\", fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d0348",
   "metadata": {},
   "source": [
    "### 3. Embedding and FAISS Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8081d9",
   "metadata": {},
   "source": [
    "The `build_vector_store` function (in lieu of rerunning the full `build_index.py` pipeline) allows you to update the FAISS index with any new or changed documents:\n",
    "\n",
    "1. **Load documents** from `data/raw` using `load_documents`.  \n",
    "2. **Embed and store** them in `data/index` via `embed_and_store` from `embedder.py`.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639010ad",
   "metadata": {},
   "source": [
    "The `embed_and_store` function handles document chunking, embedding, and FAISS index creation:\n",
    "\n",
    "1. **Chunking**: Splits each document into 512-char chunks (no overlap) using `RecursiveCharacterTextSplitter`.  \n",
    "2. **Embedding Model**: Uses `sentence-transformers/paraphrase-albert-small-v2` (normalized embeddings via `HuggingFaceEmbeddings`) for efficient, local inference.  \n",
    "3. **Index Creation**: Builds a FAISS index from the chunked texts and associated metadata.  \n",
    "4. **Persistence**: Saves the FAISS index to `data/index` for fast retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f566078",
   "metadata": {},
   "source": [
    "> ** Attention:** This process recreates the index in `data/index`.  \n",
    "> Do **not** rerun unless you have added or modified documents in `data/raw`; otherwise you may overwrite your existing index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5767bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.build_index import main as build_index\n",
    "\n",
    "# This module creates the index in the data/index directory. \n",
    "# Its not recommended to run now it again unless you change the documents in the data/raw indexing another ones in data/index directory.\n",
    "#build_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed357f5b",
   "metadata": {},
   "source": [
    "### 4. Inspect FAISS Index Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e6a77",
   "metadata": {},
   "source": [
    "The `inspect_index` function lets you peek “under the hood” of your FAISS vector store by:\n",
    "1. Loading the same embedding model used for indexing (`paraphrase-albert-small-v2` with normalized embeddings).\n",
    "2. Deserializing the FAISS index from `data/index`.\n",
    "3. Iterating over each document in the internal store to build a summary:\n",
    "   - **doc_id**: 1-based counter  \n",
    "   - **source**: original filename or URL metadata (or `\"unknown\"`)  \n",
    "   - **preview**: first ~300 characters of the document text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e09cd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 1,\n",
       "  'source': 'data\\\\raw\\\\eu_annex2_sanctions.pdf',\n",
       "  'preview': '1 \\nAnnex II – Sanctions-related commitments   \\n \\nThe sequence of implementation of the commitments d etailed in this Annex is \\nspecified in Annex V (Implementation Plan) to this Joint Comprehensive Plan of \\nAction (JCPOA). \\n  \\n \\nA.  European Union 1 \\n \\n1.  The EU and EU Member States commit to termi'},\n",
       " {'doc_id': 2,\n",
       "  'source': 'data\\\\raw\\\\eu_annex2_sanctions.pdf',\n",
       "  'preview': 'specified in Sections 1.1-1.10 below, to terminate all provisions of \\nCouncil Decision 2010/413/CFSP (as subsequently ame nded), as \\nspecified in Sections 1.1-1.10 below, and to termin ate or amend \\nnational implementing legislation as required, in a ccordance with \\nAnnex V:  \\n \\n1.1. \\n Financial, ba'},\n",
       " {'doc_id': 3,\n",
       "  'source': 'data\\\\raw\\\\eu_annex2_sanctions.pdf',\n",
       "  'preview': '30a, 30b and 31 of Council Regulation (EU) No 267/2 012); \\n \\n1.1.2. \\n Sanctions on banking activities (Article 11 of Coun cil Decision \\n2010/413/CFSP; Article 33 of Council Regulation (EU ) No 267/2012); \\n \\n1.1.3. \\n Sanctions on insurance (Article 12 of Council Decis ion \\n2010/413/CFSP; Article 35 o'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect_index import inspect_index\n",
    "\n",
    "# Load all summaries but display only the first 3 for readability\n",
    "#Note: There are many more documents in the index, but we show only the first three here to keep the output concise.\n",
    "\n",
    "summaries = inspect_index(\"data/index\")\n",
    "summaries[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352397e",
   "metadata": {},
   "source": [
    "### 5. Run a Sample QA Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10da56",
   "metadata": {},
   "source": [
    "The `backend.py` module defines how to construct and run the RAG-powered Q&A chain:\n",
    "\n",
    "1. **QA_PROMPT**  \n",
    "   - Custom prompt template that instructs the model to answer using only the provided context and to return a polite fallback if no answer is found.\n",
    "\n",
    "2. **build_qa_chain(index_path: str)**  \n",
    "   - **Loads** the FAISS index from `index_path` with the same embedding model (`paraphrase-albert-small-v2`).  \n",
    "   - **Initializes** the local Ollama LLM (`tinyllama`) with controlled temperature, top-p, and repeat penalty.  \n",
    "   - **Creates** a `RetrievalQA` chain that retrieves relevant passages and generates answers via the prompt.\n",
    "\n",
    "3. **run_qa_app(index_path: str)**  Starts a **terminal-based loop**: prompts the user for questions, invokes the QA chain, prints answers, and exits on “exit”/“quit”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023827e0",
   "metadata": {},
   "source": [
    "## Using the Copilot: CLI Terminal & Notebook \n",
    "\n",
    "**1. Terminal Q&A Session**  \n",
    "Launch the interactive CLI in your terminal (not in a notebook cell):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58981188",
   "metadata": {},
   "source": [
    "**Ask:** Type your question at the `>` prompt and press **Enter**.  \n",
    "**Answer:** You’ll see a source-cited response in the terminal.  \n",
    "**Exit:** Press **Esc** or type `exit`/`quit`.  \n",
    "**Tip:** Use the curated questions in `compliance_suggestion_questions.md` or feed questions from `test.txt` as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd5df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS index from: data/index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\Desktop\\compliance_RAG\\backend.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(\n",
      "c:\\Users\\Ana\\Desktop\\compliance_RAG\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting local LLM via Ollama ===\n",
      "--- Question 1 ---\n",
      "1. OLÁ\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ana\\Desktop\\compliance_RAG\\backend.py:36: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, based on the provided context, \"I'm sorry, I couldn't find information about that based on the current documents.\"\n",
      "\n",
      "--- Question 2 ---\n",
      "2. How are cryptocurrency mixers used in ransomware schemes?\n",
      "\n",
      "Answer: Cryptocurrency mixers are used in ransomware schemes to protect victims' anonymity by mixing their payment requests with other transactions, making it difficult for cybercriminals to trace the origin of the funds. Mixers can also be used to conceal the true identity of the victim or to create a false impression that the payment request is legitimate.\n",
      "\n",
      "--- Question 3 ---\n",
      "3. What are the risks associated with cryptocurrency mixers?\n",
      "\n",
      "The provided context mentions that the DOJ has built relationships with regulatory and enforcement partners both within the US government and around the world, and outlines their response strategies to the use of cryptocurrency as a payment method by bad actors to facilitate ransom and blackmail. The publication produced by the Attorney General’s Cyber-Digital Task Force titled \"Cryptocurrency: An Enforceable Framework\" provides a comprehensive overview of these emerging threats, including the growing proliferation of anonymous cryptocurrencies (AECs) and decentralized mixers. The publication also highlights the risks associated with these mixers, such as the use of CVCs to trade for other crime-related activities and the potential obfuscation of illicit activities by mixing them with legitimate transactions.\n",
      "\n",
      "--- Question 4 ---\n",
      "4. How are cryptocurrency mixers used in ransomware schemes?\n",
      "\n",
      "Answer: Cryptocurrency mixers are used in ransomware schemes to conceal the true origin, transfer, or destination of a ransom payment. They allow cybercriminals to hide the identity of the victim and the amount they owe, making it difficult for law enforcement agencies to track down and recover funds. In ransomware payments, cryptocurrency mixers are used to create a new virtual currency (CVC) that is then used as a means of payment. The CVC is then transferred to the victim's bank account or other designated location, where it is converted into fiat currency (i.e., money in physical form) and sent to the cybercriminal.\n",
      "\n",
      "--- Question 5 ---\n",
      "5. What best practices does FinCEN recommend for documenting EFE in SAR reports?\n",
      "\n",
      "The provided context mentions the FINCEN Advisory (12-SAREn), which recommends reporting on how perpetrators of EFE communicate with and target older adults. The advisory also includes tips for providing details about amounts involved, supporting documentation, and whether refunds were made to the older customer. The FINCEN notes that these best practices are not regulatory obligations but are best practices in regard to filing a SARE for suspected EFE and are not required by FinCEN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Running Each Question through the CLI Chain in the Notebook\n",
    "\n",
    "from backend import build_qa_chain\n",
    "\n",
    "# This module builds the question-answering chain using the index in data/index directory.\n",
    "qa_chain = build_qa_chain(\"data/index\")\n",
    "\n",
    "# Read questions from TEST.txt and run them through the QA chain\n",
    "with open('TEST.txt', 'r', encoding='utf-8') as f:\n",
    "    questions = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"--- Question {i} ---\\n{q}\\n\")\n",
    "    result = qa_chain.invoke({\"query\": q})\n",
    "    print(f\"{result['result']}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac166d3",
   "metadata": {},
   "source": [
    "\n",
    "**Discussion & Tuning**  \n",
    "- The **fallback response** on “OLÁ” shows the configured prompt and chain guard against hallucinations.  \n",
    "- **Iterative refinement:** repeating Question 2 as Question 4 produces a slightly different (and often clearer) formulation.  \n",
    "- **Model parameters** (temperature, top_p, repeat_penalty, etc.) directly influence answer style:\n",
    "  - **Higher temperature** → more varied, creative responses (but possibly less precise).  \n",
    "  - **Lower temperature** → more deterministic, focused answers.  \n",
    "  - **top_p** controls nucleus sampling; **repeat_penalty** discourages verbatim repetition.  \n",
    "\n",
    "> Tweak these settings in `Ollama(...)` or your chosen LLM config to dial in the desired balance between creativity and accuracy.\n",
    "\n",
    "**Real-World Front-End Behavior**  \n",
    "- In a production UI, users would **enter one question at a time**, not batch via a `.txt`.  \n",
    "- The system would respond **in real time**, rendering answers as:\n",
    "  - **Free-text paragraphs**  \n",
    "  - **Structured tables** or lists  \n",
    "  - **Citations** linking back to original documents  \n",
    "- A front end would also **store conversation history**, allowing follow-up queries and context-aware interactions.  \n",
    "- Users could provide **feedback** on individual answers, enabling continuous improvement and re-tuning of prompts or model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**  \n",
    "Batch testing with `test.txt` is a convenient POC shortcut. In a full deployment, the Copilot would run interactively in a web app or chat interface—one question, one answer at a time—complete with real-time rendering, history tracking, and dynamic parameter tuning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
